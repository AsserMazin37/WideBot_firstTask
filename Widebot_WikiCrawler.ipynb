{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Widebot_WikiCrawler.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4laNJF-ZB8cM",
        "colab_type": "text"
      },
      "source": [
        "**HERE IS THE FIRST TASK RECIEVED FROM WIDEBOT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koVmPbzzAWWP",
        "colab_type": "text"
      },
      "source": [
        "Let's import the required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kot1lDRKPKdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import urllib\n",
        "import time\n",
        "import requests"
      ],
      "execution_count": 492,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lRkNVCQDhBe",
        "colab_type": "text"
      },
      "source": [
        "Set our starting and target urls\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJZz8RE5VN70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_url = \"https://en.wikipedia.org/wiki/Special:Random\"\n",
        "\n",
        "target_url = \"https://en.wikipedia.org/wiki/Philosophy\"\n",
        " \n",
        "all_urls = [start_url] # store all urls in a list"
      ],
      "execution_count": 493,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxq4LCFyEe-i",
        "colab_type": "text"
      },
      "source": [
        "The find_first func gets request to the specified url then parses the text in that url by using BeautifulSoup library and then it loops in the paragraph of the article and tries to find a link within it, if found it builds a new url with this link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5owFMGgbd-ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_first(url):\n",
        "    response = requests.get(url)\n",
        "    html = response.text\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    # start with the body of the article provided by the url \n",
        "    content_div = soup.find(id=\"mw-content-text\").find(class_=\"mw-parser-output\")\n",
        "\n",
        "    article_link = None\n",
        "\n",
        "    # Find all the direct children of content_div that are paragraphs\n",
        "    for element in content_div.find_all(\"p\", recursive=False):\n",
        "        #find only the direct children\n",
        "        if element.find(\"a\", recursive=False):\n",
        "            article_link = element.find(\"a\", recursive=False).get('href')\n",
        "            break\n",
        "    \n",
        "    # if the article_link is still None then return it\n",
        "    if not article_link:\n",
        "        return\n",
        "\n",
        "    # Build the new url if the article_link is not empty\n",
        "    first_link = urllib.parse.urljoin('https://en.wikipedia.org/', article_link)\n",
        "\n",
        "    return first_link"
      ],
      "execution_count": 494,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ykr2yfLIwjR",
        "colab_type": "text"
      },
      "source": [
        "The continue_scraping func checks if we arrived at the target url or if we've reached our maximum number of iterations or if we had already visited a link before (the urls_history parameter is a list with all links visited from the beginning) and finally if nothing from all these conditions had been fulfilled then we are goof to continue scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StBwLWuI_TR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def continue_scraping(urls_history, target_url, max_steps=50):\n",
        "    \n",
        "    # if reached philosphy\n",
        "    if urls_history[-1] == target_url:\n",
        "        print(\"Target ('Philosphy') article reached!\")\n",
        "        return False\n",
        "\n",
        "    # if reached maximum iterations \n",
        "    elif len(urls_history) > max_steps:\n",
        "        print(\"Reached maximum searches, abandoning crawling!\")\n",
        "        return False\n",
        "\n",
        "    # if we had already visited this link before while scraping\n",
        "    elif urls_history[-1] in urls_history[:-1]:\n",
        "        print(\"We visited this link before, abandoning crawling!\")\n",
        "        return False\n",
        "    else:\n",
        "        return True"
      ],
      "execution_count": 495,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFcEo9SpeEBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "0a34c8c8-864f-434b-a525-c921257e338e"
      },
      "source": [
        "while continue_scraping(all_urls, target_url):\n",
        "    print(all_urls[-1])\n",
        "    first_link = find_first(all_urls[-1])\n",
        "\n",
        "    # if we arrived with an article with no links inside\n",
        "    if not first_link:\n",
        "        print(\"Arrived at an article with no links, abandoning crawling!\")\n",
        "        break\n",
        "\n",
        "    all_urls.append(first_link)\n",
        "\n",
        "    time.sleep(0.5)  # Slow things down so as to not overload Wikipedia's servers"
      ],
      "execution_count": 496,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://en.wikipedia.org/wiki/Special:Random\n",
            "https://en.wikipedia.org/wiki/Tennis\n",
            "https://en.wikipedia.org/wiki/Racket_sport\n",
            "https://en.wikipedia.org/wiki/Game\n",
            "https://en.wikipedia.org/wiki/Play_(activity)\n",
            "https://en.wikipedia.org/wiki/Motivation#Incentive_theories:_intrinsic_and_extrinsic_motivation\n",
            "https://en.wikipedia.org/wiki/Desire\n",
            "https://en.wikipedia.org/wiki/Emotion\n",
            "https://en.wikipedia.org/wiki/Biological\n",
            "https://en.wikipedia.org/wiki/Natural_science\n",
            "https://en.wikipedia.org/wiki/Branches_of_science\n",
            "https://en.wikipedia.org/wiki/Empirical\n",
            "https://en.wikipedia.org/wiki/Information\n",
            "https://en.wikipedia.org/wiki/Uncertainty\n",
            "https://en.wikipedia.org/wiki/Epistemic\n",
            "https://en.wikipedia.org/wiki/Greek_language\n",
            "Arrived at an article with no links, abandoning crawling!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcZ5wMKhG4fu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 496,
      "outputs": []
    }
  ]
}